#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
APIæ–‡æ¡£è§£æå™¨æ¨¡å—
æ”¯æŒè§£æä¸åŒæ ¼å¼çš„APIæ–‡æ¡£
"""

from .swagger import SwaggerParser
from .openapi import OpenAPIParser
from .markdown import MarkdownParser
from .crawler import CrawlerParser
from utils.logger import get_logger

logger = get_logger()


def parse_document(input_path, format_type='auto'):
    """
    è§£æAPIæ–‡æ¡£

    Args:
        input_path: æ–‡æ¡£è·¯å¾„ï¼Œå¯ä»¥æ˜¯URLæˆ–æ–‡ä»¶è·¯å¾„
        format_type: æ–‡æ¡£æ ¼å¼ï¼Œå¯ä»¥æ˜¯'swagger', 'openapi', 'markdown', 'html'æˆ–'auto'

    Returns:
        è§£æåçš„APIæ•°æ®
    """
    # æ ¹æ®æ–‡æ¡£æ ¼å¼é€‰æ‹©åˆé€‚çš„è§£æå™¨
    if format_type == 'auto':
        # è‡ªåŠ¨æ£€æµ‹æ–‡æ¡£æ ¼å¼
        format_type = detect_format(input_path)

    parser_map = {
        'swagger': SwaggerParser,
        'openapi': OpenAPIParser,
        'markdown': MarkdownParser,
        'html': CrawlerParser,
        'crawler': CrawlerParser
    }

    if format_type not in parser_map:
        logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„æ–‡æ¡£æ ¼å¼: {format_type}ï¼Œå°è¯•ä½¿ç”¨çˆ¬è™«è§£æ")
        format_type = 'crawler'

    logger.info(f"ğŸ“„ ä½¿ç”¨ {format_type} è§£æå™¨è§£ææ–‡æ¡£")
    parser_class = parser_map[format_type]
    parser = parser_class()

    try:
        # è§£ææ–‡æ¡£
        result = parser.parse(input_path)

        # æ£€æŸ¥è§£ææ˜¯å¦æˆåŠŸ
        if result and (
            (isinstance(result, dict) and ('paths' in result or 'endpoints' in result))
            or (isinstance(result, list) and len(result) > 0)
        ):
            logger.info(f"âœ… æˆåŠŸè§£ææ–‡æ¡£ï¼Œä½¿ç”¨ {format_type} æ ¼å¼")
            return result
        else:
            # å¦‚æœè§£æç»“æœä¸ºç©ºï¼Œå°è¯•ä½¿ç”¨çˆ¬è™«è§£æ
            if format_type != 'crawler':
                logger.warning(f"âš ï¸ {format_type} è§£æç»“æœä¸ºç©ºï¼Œå°è¯•ä½¿ç”¨çˆ¬è™«è§£æ")
                crawler = CrawlerParser()
                crawler_result = crawler.parse(input_path)

                # æ£€æŸ¥çˆ¬è™«è§£ææ˜¯å¦æˆåŠŸ
                if crawler_result and 'paths' in crawler_result and crawler_result['paths']:
                    logger.info("âœ… æˆåŠŸä½¿ç”¨çˆ¬è™«è§£ææ–‡æ¡£")
                    return crawler_result

            logger.warning("âš ï¸ è§£æç»“æœä¸ºç©º")
            return result
    except Exception as e:
        logger.error(f"âŒ è§£ææ–‡æ¡£å‡ºé”™: {str(e)}")

        # å¦‚æœä¸æ˜¯çˆ¬è™«è§£æï¼Œå°è¯•ä½¿ç”¨çˆ¬è™«è§£æ
        if format_type != 'crawler':
            logger.info("ğŸ”„ å°è¯•ä½¿ç”¨çˆ¬è™«è§£ææ–‡æ¡£")
            try:
                crawler = CrawlerParser()
                crawler_result = crawler.parse(input_path)

                # æ£€æŸ¥çˆ¬è™«è§£ææ˜¯å¦æˆåŠŸ
                if crawler_result and 'paths' in crawler_result and crawler_result['paths']:
                    logger.info("âœ… æˆåŠŸä½¿ç”¨çˆ¬è™«è§£ææ–‡æ¡£")
                    return crawler_result
            except Exception as crawler_error:
                logger.error(f"âŒ çˆ¬è™«è§£ææ–‡æ¡£å‡ºé”™: {str(crawler_error)}")

        # é‡æ–°æŠ›å‡ºåŸå§‹å¼‚å¸¸
        raise


def detect_format(input_path):
    """
    è‡ªåŠ¨æ£€æµ‹æ–‡æ¡£æ ¼å¼

    Args:
        input_path: æ–‡æ¡£è·¯å¾„

    Returns:
        æ£€æµ‹åˆ°çš„æ–‡æ¡£æ ¼å¼
    """
    # æ ¹æ®æ–‡ä»¶æ‰©å±•åæˆ–å†…å®¹åˆ¤æ–­æ–‡æ¡£æ ¼å¼
    if input_path.lower().endswith(('.json', '.yaml', '.yml')):
        # å°è¯•åŒºåˆ†Swaggerå’ŒOpenAPI
        try:
            import json
            import yaml
            import requests

            # åŠ è½½æ–‡æ¡£å†…å®¹
            if input_path.startswith(('http://', 'https://')):
                response = requests.get(input_path)
                content = response.text
            else:
                with open(input_path, 'r', encoding='utf-8') as f:
                    content = f.read()

            # è§£æå†…å®¹
            if input_path.lower().endswith('.json'):
                data = json.loads(content)
            else:
                data = yaml.safe_load(content)

            # æ£€æŸ¥æ˜¯å¦ä¸ºSwagger 2.0
            if 'swagger' in data and data['swagger'] == '2.0':
                return 'swagger'

            # æ£€æŸ¥æ˜¯å¦ä¸ºOpenAPI 3.x
            if 'openapi' in data and data['openapi'].startswith('3.'):
                return 'openapi'

            # è¿”å›é»˜è®¤å€¼
            return 'openapi'
        except Exception:
            # è§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
            return 'openapi'
    elif input_path.lower().endswith('.md'):
        return 'markdown'
    elif input_path.lower().endswith(('.html', '.htm')):
        return 'html'
    elif input_path.startswith(('http://', 'https://')):
        # å¯¹äºURLï¼Œå°è¯•æ£€æµ‹å†…å®¹ç±»å‹
        try:
            import requests
            response = requests.head(input_path)
            content_type = response.headers.get('Content-Type', '')

            if 'json' in content_type:
                # å°è¯•è·å–å¹¶è§£æJSON
                response = requests.get(input_path)
                data = response.json()

                # æ£€æŸ¥æ˜¯å¦ä¸ºSwagger 2.0æˆ–OpenAPI 3.x
                if 'swagger' in data and data['swagger'] == '2.0':
                    return 'swagger'
                elif 'openapi' in data and data['openapi'].startswith('3.'):
                    return 'openapi'

                return 'openapi'
            elif 'text/html' in content_type:
                return 'html'
            elif 'text/markdown' in content_type:
                return 'markdown'
        except Exception:
            # å¦‚æœæ£€æµ‹å¤±è´¥ï¼Œé»˜è®¤ä½¿ç”¨çˆ¬è™«è§£æ
            return 'crawler'

    # é»˜è®¤ä½¿ç”¨çˆ¬è™«å°è¯•è§£æ
    return 'crawler'
