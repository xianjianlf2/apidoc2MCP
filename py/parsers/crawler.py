
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import re
import requests
from bs4 import BeautifulSoup, NavigableString, Tag
from urllib.parse import urlparse
from .base import BaseParser
from utils.logger import get_logger

logger = get_logger()


class CrawlerParser(BaseParser):
    """
    APIæ–‡æ¡£çˆ¬è™«è§£æå™¨ç±»
    å°è¯•ä»HTMLæ–‡æ¡£ä¸­ç»“æ„åŒ–åœ°æå–APIæ¥å£æˆ–æ“ä½œä¿¡æ¯ã€‚
    """

    def __init__(self):
        """åˆå§‹åŒ–çˆ¬è™«è§£æå™¨"""
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        # ç”¨äºå­˜å‚¨æå–åˆ°çš„ç«¯ç‚¹/æ“ä½œä¿¡æ¯
        self.extracted_operations = []
        # å¸¸è§çš„å¯èƒ½åŒ…å«APIæˆ–æ“ä½œä¿¡æ¯çš„HTMLå…ƒç´ é€‰æ‹©å™¨
        # å¢åŠ äº†å¯¹ä»£ç å—ã€æ ‡é¢˜ç­‰çš„å…³æ³¨
        self.operation_selectors = [
            'pre', 'code',  # ä»£ç å—ä¼˜å…ˆ
            'div.endpoint', 'section.api', 'article.endpoint',  # ç‰¹å®šå®¹å™¨
            'table.api', 'table.endpoints',  # è¡¨æ ¼
            'h2', 'h3', 'h4'  # æ ‡é¢˜ï¼ˆåç»­ä¼šåˆ†æå…¶å†…å®¹å’Œç›¸é‚»å…ƒç´ ï¼‰
        ]
        # ç”¨äºæå–URLè·¯å¾„ã€HTTPæ–¹æ³•ã€å‘½ä»¤è¡Œæ¨¡å¼çš„æ­£åˆ™è¡¨è¾¾å¼
        # ç¨å¾®æ”¾å®½è·¯å¾„åŒ¹é…ï¼Œå…è®¸æ›´å¤æ‚çš„å­—ç¬¦
        self.path_pattern = re.compile(r'((?:/[a-zA-Z0-9{}/?=&%.:\-_~]+)+)')
        self.method_pattern = re.compile(
            r'\b(GET|POST|PUT|DELETE|PATCH|OPTIONS|HEAD)\b', re.IGNORECASE)
        self.cmd_pattern = re.compile(
            r'^(?:python|pip|mcp|docker|kubectl|aws|gcloud|az|.\/|bash)\s+.*', re.IGNORECASE)
        self.param_pattern = re.compile(r'\{([^}]+)\}')  # è·¯å¾„å‚æ•° {param}
        # å‘½ä»¤è¡Œå‚æ•° --arg=<val>, -a, <placeholder>
        self.arg_pattern = re.compile(
            r'(--[a-zA-Z0-9\-]+(?:=[^ ]+)?)|(-[a-zA-Z][a-zA-Z0-9\-]*)|(<[^>]+>)')

    def parse(self, input_path):
        """
        è§£æAPIæ–‡æ¡£ç½‘é¡µæˆ–æœ¬åœ°HTMLæ–‡ä»¶

        Args:
            input_path: æ–‡æ¡£URLæˆ–æœ¬åœ°HTMLæ–‡ä»¶è·¯å¾„

        Returns:
            åŒ…å«æå–åˆ°çš„æ“ä½œä¿¡æ¯çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—å…¸ã€‚
            ä¾‹å¦‚: [{'type': 'http', 'path': '/users/{id}', 'method': 'GET', 'description': '...', 'parameters': [...]},
                   {'type': 'cmd', 'command': 'python main.py ...', 'description': '...', 'parameters': [...]}]
            å¦‚æœæ— æ³•è§£ææˆ–æå–ï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚
        """
        logger.info(f"ğŸ•¸ï¸ å¼€å§‹ä» {input_path} æŠ“å–æ“ä½œä¿¡æ¯")
        self.extracted_operations = []  # é‡ç½®

        try:
            content = self.load_content(input_path)
            soup = BeautifulSoup(content, 'html.parser')

            # 1. æå–é¡µé¢åŸºæœ¬ä¿¡æ¯ (å¯é€‰ï¼Œæš‚ä¸çº³å…¥æ ¸å¿ƒæå–ç»“æœ)
            page_info = self._extract_page_info(soup)
            logger.info(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {page_info.get('title', 'N/A')}")

            # 2. ç»“æ„åŒ–æå–
            self._structured_extraction(soup)

            # 3. å¦‚æœç»“æ„åŒ–æå–æ•ˆæœä¸ä½³ï¼Œå°è¯•åŸºäºæ–‡æœ¬çš„æå– (ä½œä¸ºè¡¥å……)
            if not self.extracted_operations:
                logger.info("ğŸ¤” ç»“æ„åŒ–æå–æœªæ‰¾åˆ°æ˜ç¡®æ“ä½œï¼Œå°è¯•åŸºäºæ–‡æœ¬å†…å®¹æå–...")
                self._text_based_extraction(soup)

            if not self.extracted_operations:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å¯è¯†åˆ«çš„æ“ä½œæˆ–ç«¯ç‚¹ä¿¡æ¯")
            else:
                logger.info(
                    f"âœ… æå–åˆ° {len(self.extracted_operations)} ä¸ªå¯èƒ½çš„æ“ä½œ/ç«¯ç‚¹")

            return self.extracted_operations

        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ HTTPè¯·æ±‚å¤±è´¥: {e}")
            return []
        except FileNotFoundError:
            logger.error(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {input_path}")
            return []
        except Exception as e:
            logger.error(f"âŒ è§£æHTMLæ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            return []

    def _extract_page_info(self, soup):
        """æå–é¡µé¢æ ‡é¢˜ç­‰åŸºæœ¬ä¿¡æ¯"""
        info = {}
        title_tag = soup.find('title')
        if title_tag and title_tag.string:
            info['title'] = title_tag.string.strip()
        # å¯ä»¥æ‰©å±•æå– meta description ç­‰
        return info

    def _structured_extraction(self, soup):
        """å°è¯•é€šè¿‡å¸¸è§çš„HTMLç»“æ„æå–ä¿¡æ¯"""
        processed_elements = set()  # é˜²æ­¢é‡å¤å¤„ç†

        for selector in self.operation_selectors:
            elements = soup.select(selector)
            for element in elements:
                # è·³è¿‡å·²ç»å¤„ç†è¿‡çš„å…ƒç´ åŠå…¶å­å…ƒç´ 
                if any(processed_el in element.parents or processed_el == element for processed_el in processed_elements):
                    continue

                logger.debug(
                    f"ğŸ” æ­£åœ¨å¤„ç†å…ƒç´ : <{element.name}> (Selector: {selector})")
                extracted = False
                if element.name in ['pre', 'code']:
                    extracted = self._process_code_block(element)
                elif element.name == 'table':
                    extracted = self._process_table(element)
                elif element.name.startswith('h'):  # å¤„ç†æ ‡é¢˜åŠå…¶åç»­å†…å®¹
                    extracted = self._process_heading_section(element)
                elif element.name in ['div', 'section', 'article']:  # å¤„ç†é€šç”¨å®¹å™¨
                    extracted = self._process_container(element)

                if extracted:
                    processed_elements.add(element)  # æ ‡è®°ä¸ºå·²å¤„ç†

    def _text_based_extraction(self, soup):
        """ä»é¡µé¢çº¯æ–‡æœ¬ä¸­æå–ä¿¡æ¯ (æ•ˆæœå¯èƒ½è¾ƒå·®)"""
        # å®ç°åŸºäºæ­£åˆ™è¡¨è¾¾å¼çš„ç®€å•æ–‡æœ¬æå–é€»è¾‘ï¼ˆæ­¤å¤„çœç•¥å…·ä½“å®ç°ç»†èŠ‚ï¼‰
        # è¿™ç§æ–¹æ³•å‡†ç¡®ç‡è¾ƒä½ï¼Œå®¹æ˜“è¯¯åˆ¤ï¼Œä»…ä½œä¸ºè¡¥å……
        pass

    def _process_code_block(self, element):
        """å¤„ç† <pre> æˆ– <code> å—"""
        text = element.get_text("\n", strip=True)
        lines = text.split('\n')
        extracted_count = 0
        for line in lines:
            line = line.strip()
            if not line:
                continue

            # å°è¯•è¯†åˆ«HTTPè¯·æ±‚ç¤ºä¾‹ (curl, httpie, etc.) æˆ– APIè·¯å¾„/æ–¹æ³•
            method = self._find_method(line)
            path = self._find_path(line)
            cmd_match = self.cmd_pattern.match(line)

            if cmd_match:
                operation = self._create_operation_entry('cmd', command=line)
                # å°è¯•ä»ç›¸é‚»å…ƒç´ è·å–æè¿°
                desc = self._find_neighbor_description(element)
                if desc:
                    operation['description'] = desc
                # æå–å‚æ•°
                operation['parameters'] = self._extract_cmd_args(line)
                self.extracted_operations.append(operation)
                logger.debug(f"  -> æå–åˆ° CMD: {line[:80]}...")
                extracted_count += 1
            elif path:
                operation = self._create_operation_entry(
                    'http', path=path, method=(method if method else 'get'))
                # å°è¯•ä»ç›¸é‚»å…ƒç´ è·å–æè¿°
                desc = self._find_neighbor_description(element)
                if desc:
                    operation['description'] = desc
                # æå–å‚æ•°
                operation['parameters'] = self._extract_http_params(
                    path, element.get_text())
                self.extracted_operations.append(operation)
                logger.debug(
                    f"  -> æå–åˆ° HTTP (from code): {operation.get('method','').upper()} {path}")
                extracted_count += 1
            elif method:  # å¦‚æœåªæ‰¾åˆ°æ–¹æ³•ï¼Œä¹Ÿè®°å½•ä¸‹æ¥ï¼Œå¯èƒ½åç»­æœ‰è·¯å¾„
                operation = self._create_operation_entry(
                    'http', method=method, description=f"Found method '{method}' in code block: {line[:50]}...")
                self.extracted_operations.append(operation)
                logger.debug(f"  -> æå–åˆ°æ½œåœ¨ HTTP Method (from code): {method}")
                extracted_count += 1
        return extracted_count > 0

    def _process_table(self, element):
        """å¤„ç† <table> å…ƒç´ """
        # æå–è¡¨å¤´å’Œæ•°æ®è¡Œï¼Œåˆ†ææ¯ä¸€è¡Œå°è¯•ç»„åˆæˆAPIä¿¡æ¯
        # (çœç•¥è¯¦ç»†å®ç°ï¼Œé€»è¾‘ä¼šæ¯”è¾ƒå¤æ‚ï¼Œéœ€è¦åˆ¤æ–­åˆ—çš„å«ä¹‰)
        logger.debug("  -> è·³è¿‡è¡¨æ ¼å¤„ç† (æœªå®ç°)")
        return False  # æ ‡è®°ä¸ºæœªå¤„ç†

    def _process_heading_section(self, element):
        """å¤„ç†æ ‡é¢˜ (h2-h4) åŠå…¶ç´§éšå…¶åçš„å†…å®¹"""
        heading_text = element.get_text(strip=True)
        method = self._find_method(heading_text)
        path = self._find_path(heading_text)
        cmd_match = self.cmd_pattern.match(heading_text)

        # å¦‚æœæ ‡é¢˜æœ¬èº«åŒ…å«APIä¿¡æ¯
        if path or cmd_match:
            description_parts = []
            parameters = []
            # æ”¶é›†ç´§éšå…¶åçš„æè¿°æ€§å†…å®¹ (<p>, <ul>, <ol>, <pre>, <code>)
            sibling = element.find_next_sibling()
            while sibling and isinstance(sibling, Tag) and sibling.name not in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                if sibling.name == 'p':
                    description_parts.append(sibling.get_text(strip=True))
                elif sibling.name in ['ul', 'ol']:
                    # å°è¯•ä»åˆ—è¡¨ä¸­æå–å‚æ•°
                    for li in sibling.find_all('li'):
                        param_text = li.get_text(strip=True)
                        # ç®€å•çš„å‚æ•°æ ¼å¼æ£€æµ‹ (e.g., "param_name: description")
                        match = re.match(
                            r'^([\w\-]+)\s*[:\-]\s*(.*)', param_text)
                        if match:
                            param_name, param_desc = match.groups()
                            parameters.append(
                                {'name': param_name.strip(), 'description': param_desc.strip()})
                        else:
                            description_parts.append(param_text)  # å¦åˆ™è§†ä¸ºæè¿°
                elif sibling.name in ['pre', 'code']:
                    # å¦‚æœåé¢ç´§è·Ÿç€ä»£ç å—ï¼Œä¹ŸåŠ å…¥æè¿°æˆ–å°è¯•æå–æ›´å¤šä¿¡æ¯
                    description_parts.append(
                        f"Example:\n{sibling.get_text(strip=True)}")
                    # å¯ä»¥è°ƒç”¨ _process_code_block å¤„ç†è¿™ä¸ªä»£ç å—ï¼Œä½†è¦æ³¨æ„é¿å…é‡å¤æ·»åŠ 
                sibling = sibling.find_next_sibling()

            description = "\n".join(description_parts).strip()

            if cmd_match:
                operation = self._create_operation_entry(
                    'cmd', command=heading_text, description=description)
                operation['parameters'] = self._extract_cmd_args(
                    heading_text) + parameters  # åˆå¹¶æå–åˆ°çš„å‚æ•°
                self.extracted_operations.append(operation)
                logger.debug(f"  -> æå–åˆ° CMD (from heading): {heading_text}")
                return True
            elif path:
                operation = self._create_operation_entry('http', path=path, method=(
                    method if method else 'get'), description=description)
                operation['parameters'] = self._extract_http_params(
                    path, description) + parameters  # åˆå¹¶
                self.extracted_operations.append(operation)
                logger.debug(
                    f"  -> æå–åˆ° HTTP (from heading): {operation.get('method','').upper()} {path}")
                return True
        return False

    def _process_container(self, element):
        """å¤„ç† <div>, <section>, <article> ç­‰å®¹å™¨å…ƒç´ """
        # å°è¯•åœ¨å®¹å™¨å†…éƒ¨æŸ¥æ‰¾APIç‰¹å¾ï¼Œä¾‹å¦‚æŸ¥æ‰¾åŒ…å«HTTPæ–¹æ³•å’Œè·¯å¾„çš„æ–‡æœ¬å—
        text_content = element.get_text(" ", strip=True)
        method = self._find_method(text_content)
        path = self._find_path(text_content)

        if method and path:
            # å¦‚æœå®¹å™¨å†…åŒæ—¶åŒ…å«æ–¹æ³•å’Œè·¯å¾„ï¼Œå¯èƒ½æ˜¯ä¸€ä¸ªAPIæè¿°å—
            description = text_content  # ä½¿ç”¨æ•´ä¸ªå®¹å™¨æ–‡æœ¬ä½œä¸ºåˆæ­¥æè¿°
            operation = self._create_operation_entry(
                'http', path=path, method=method, description=description)
            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤æ‚çš„é€»è¾‘æ¥æŸ¥æ‰¾å®¹å™¨å†…çš„å‚æ•°æè¿°
            operation['parameters'] = self._extract_http_params(
                path, text_content)
            self.extracted_operations.append(operation)
            logger.debug(
                f"  -> æå–åˆ° HTTP (from container): {method.upper()} {path}")
            return True  # æ ‡è®°æ­¤å®¹å™¨å·²å¤„ç†

        # ä¹Ÿå¯ä»¥é€’å½’å¤„ç†å®¹å™¨å†…çš„å­å…ƒç´ ï¼Œä½†è¦æ³¨æ„æ•ˆç‡å’Œé¿å…é‡å¤
        # for child in element.find_all(recursive=False):
        #     if isinstance(child, Tag):
        #         self._structured_extraction(child) # ç®€åŒ–å¤„ç†ï¼Œå¯èƒ½éœ€è¦æ›´å¤æ‚çš„é€»è¾‘

        return False

    def _find_method(self, text):
        """åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾HTTPæ–¹æ³•"""
        match = self.method_pattern.search(text)
        return match.group(1).lower() if match else None

    def _find_path(self, text):
        """åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾å¯èƒ½çš„APIè·¯å¾„"""
        # ä¼˜å…ˆåŒ¹é…æ›´é•¿çš„ã€åŒ…å«å¤šä¸ªæ–œæ çš„è·¯å¾„
        matches = self.path_pattern.findall(text)
        if not matches:
            return None
        # é€‰æ‹©çœ‹èµ·æ¥æœ€åƒAPIè·¯å¾„çš„åŒ¹é…é¡¹ï¼ˆä¾‹å¦‚ï¼ŒåŒ…å«å¤šä¸ªæ–œæ ï¼‰
        best_match = max(matches, key=lambda p: p.count('/'))
        # è¿‡æ»¤æ‰ä¸€äº›æ˜æ˜¾ä¸æ˜¯è·¯å¾„çš„ï¼ˆæ¯”å¦‚åªæœ‰ä¸€ä¸ªæ–œæ ä¸”å¾ˆçŸ­ï¼‰
        if best_match.count('/') >= 1 and len(best_match) > 2:
            # ç§»é™¤å¯èƒ½çš„å°¾éšæ ‡ç‚¹
            best_match = re.sub(r'[.,;:)\s]+$', '', best_match)
            return best_match
        return None

    def _find_neighbor_description(self, element):
        """å°è¯•æŸ¥æ‰¾å…ƒç´ å‰é¢æˆ–åé¢çš„æè¿°æ€§æ–‡æœ¬"""
        desc_parts = []
        # å‘å‰æŸ¥æ‰¾ P æ ‡ç­¾
        prev = element.find_previous_sibling()
        if prev and isinstance(prev, Tag) and prev.name == 'p':
            desc_parts.insert(0, prev.get_text(strip=True))
        # å‘åæŸ¥æ‰¾ P æ ‡ç­¾
        next_el = element.find_next_sibling()
        if next_el and isinstance(next_el, Tag) and next_el.name == 'p':
            desc_parts.append(next_el.get_text(strip=True))
        return "\n".join(desc_parts) if desc_parts else None

    def _extract_http_params(self, path, context_text):
        """ä»è·¯å¾„å’Œä¸Šä¸‹æ–‡æ–‡æœ¬ä¸­æå–HTTPå‚æ•°"""
        params = []
        # 1. æå–è·¯å¾„å‚æ•°
        path_params = self.param_pattern.findall(path)
        for p_name in path_params:
            params.append({
                'name': p_name,
                'in': 'path',
                'required': True,
                'description': f'Path parameter: {p_name}',
                'schema': {'type': 'string'}  # é»˜è®¤ç±»å‹
            })

        # 2. TODO: ä»ä¸Šä¸‹æ–‡æ–‡æœ¬ä¸­æå–æŸ¥è¯¢å‚æ•°ã€è¯·æ±‚ä½“å‚æ•°ç­‰ (éœ€è¦æ›´å¤æ‚çš„NLPæˆ–è§„åˆ™)
        # ä¾‹å¦‚æŸ¥æ‰¾ "Parameters:", "Query Parameters:", "Request Body:", æˆ–è¡¨æ ¼/åˆ—è¡¨ä¸­çš„å‚æ•°æè¿°
        return params

    def _extract_cmd_args(self, command_line):
        """ä»å‘½ä»¤è¡Œæ–‡æœ¬ä¸­æå–å‚æ•°"""
        args = []
        matches = self.arg_pattern.findall(command_line)
        # findall è¿”å›çš„æ˜¯å…ƒç»„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç»„å¯¹åº”ä¸€ä¸ªæ•è·ç»„
        for match_tuple in matches:
            arg = next((m for m in match_tuple if m), None)  # æ‰¾åˆ°éç©ºçš„åŒ¹é…é¡¹
            if arg:
                param_name = arg
                description = f"Command-line argument: {arg}"
                is_placeholder = arg.startswith('<') and arg.endswith('>')
                is_option = arg.startswith('-')

                if is_option:
                    param_name = arg.split('=', 1)[0]  # å¤„ç† --arg=value å½¢å¼
                    description = f"Command-line option: {param_name}"
                elif is_placeholder:
                    param_name = arg[1:-1]  # ç§»é™¤å°–æ‹¬å·
                    description = f"Command-line placeholder: {param_name}"

                args.append({
                    'name': param_name,
                    'in': 'argument' if not is_option else 'option',
                    'required': is_placeholder,  # ç®€å•å‡è®¾å ä½ç¬¦æ˜¯å¿…éœ€çš„
                    'description': description,
                    'schema': {'type': 'string'}
                })
        return args

    def _create_operation_entry(self, op_type, **kwargs):
        """åˆ›å»ºæ ‡å‡†åŒ–çš„æ“ä½œæ¡ç›®å­—å…¸"""
        entry = {'type': op_type}
        entry.update(kwargs)
        return entry

# --- Helper for loading content (kept from BaseParser logic) ---
    def is_url(self, input_path):
        """åˆ¤æ–­è¾“å…¥æ˜¯å¦ä¸ºURL"""
        return input_path.startswith('http://') or input_path.startswith('https://')

    def load_content(self, input_path):
        """åŠ è½½æ–‡æ¡£å†…å®¹"""
        if self.is_url(input_path):
            logger.debug(f"  -> å‘é€HTTP GETè¯·æ±‚åˆ°: {input_path}")
            response = requests.get(
                input_path, headers=self.headers, timeout=15)
            response.raise_for_status()
            # å°è¯•æ£€æµ‹ç¼–ç ï¼Œä¼˜å…ˆä½¿ç”¨ headersï¼Œç„¶åæ˜¯ meta charsetï¼Œæœ€åæ˜¯ chardet
            content_type = response.headers.get('content-type', '').lower()
            charset = requests.utils.get_encoding_from_headers(
                response.headers)
            if not charset and 'text/html' in content_type:
                # æ£€æŸ¥ meta charset
                # Use a lenient initial parse
                soup = BeautifulSoup(
                    response.content, 'html.parser', from_encoding='iso-8859-1')
                meta_charset = soup.find('meta', charset=True)
                if meta_charset:
                    charset = meta_charset['charset']
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œå°è¯• chardet (éœ€è¦å®‰è£… chardet åº“)
            # if not charset:
            #     try:
            #         import chardet
            #         charset = chardet.detect(response.content)['encoding']
            #     except ImportError:
            #         pass # chardet not installed

            # ä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç æˆ–é»˜è®¤ UTF-8 è§£ç 
            response.encoding = charset if charset else 'utf-8'
            logger.debug(f"  -> ä½¿ç”¨ç¼–ç  '{response.encoding}' è§£ç å†…å®¹")
            return response.text
        else:
            logger.debug(f"  -> è¯»å–æœ¬åœ°æ–‡ä»¶: {input_path}")
            # è¯»å–æœ¬åœ°æ–‡ä»¶æ—¶å°è¯•è‡ªåŠ¨æ£€æµ‹ç¼–ç 
            try:
                with open(input_path, 'r', encoding='utf-8') as f:
                    return f.read()
            except UnicodeDecodeError:
                logger.warning(f"âš ï¸ UTF-8è§£ç å¤±è´¥ï¼Œå°è¯•GBKç¼–ç : {input_path}")
                try:
                    with open(input_path, 'r', encoding='gbk') as f:
                        return f.read()
                except Exception as e:
                    logger.error(f"âŒ ä½¿ç”¨GBKç¼–ç è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
                    raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸
